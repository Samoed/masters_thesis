#+STARTUP: latexpreview
#+TITLE: Разработка фреймфорка по автоматическому определению интентов
#+AUTHOR: Соломатин Роман Игоревич
#+LANGUAGE: ru
#+cite_export: biblatex
#+COMMENT: ':t for https://stackoverflow.com/questions/15097114/how-to-get-smart-quotes-on-org-mode-export
#+LATEX_CLASS: ITMOMasters
#+LATEX_CLASS_OPTIONS: [14pt,a4paper,oneside,openany]
#+LATEX_HEADER: \usepackage[T2A]{fontenc}
#+LATEX_HEADER_EXTRA: \include{config}
#+LATEX_HEADER_EXTRA: \renewcommand{\contentsname}{Содержание}
#+OPTIONS: date:nil

* ТЕРМИНЫ И ОПРЕДЕЛЕНИЯ
:PROPERTIES:
:UNNUMBERED: t
:END:
*AutoML* -- автоматическое машинное обучение.

*NLP* -- обработка естественного языка.

*NLU* -- понимание естественного языка.

*Эмбеддинг* -- .

*Промпт* -- .

Инференс

* Введение
:PROPERTIES:
:UNNUMBERED: t
:END:
#+LATEX: \addcontentsline{toc}{chapter}{Введение}
В последние годы наблюдается бурный рост интереса к диалоговым системам на основе искусственного интеллекта (чат-ботам, голосовым помощникам и так далее). Так, по данным трендов, интерес к голосовым технологиям AI вырос почти втрое за пять лет[fn:1]. Диалоговые системы внедряются в бизнес-процессы и повседневную жизнь, однако создание их интеллектуальной части – модели определения намерения пользователя (intent classification) – остается сложной задачей. Такой модуль является ключевым компонентом системы, позволяя автоматически выявлять цель запроса пользователя, но учет специфики различных доменов серьезно затрудняет разработку универсальной модели. Проектирование и тонкая настройка модели интентов требуют значительных экспертных усилий в области NLP и ML. Поэтому актуальной представляется автоматизация данного процесса – создание универсальных решений, способных уменьшить долю ручной работы и упростить разработку моделей классификации интентов. Автоматизированные подходы к машинному обучению (AutoML) обещают значительно сократить объем ручного труда за счет автоматического подбора оптимальных моделей и параметров, что особенно важно для быстро растущей области диалоговых систем.

На сегодняшний день для задачи классификации интентов накоплен внушительный арсенал методов. Традиционно применяются алгоритмы классического машинного обучения, такие как наивный tf-idf[cite:@joneskarensparck_statistical_1972], а также подходы на основе k-ближайших соседей и ансамблевые методы (например, градиентный бустинг). С развитием глубокого обучения все более широко используются нейросетевые модели, таких как BERT[cite:@devlin_bert_2019], которые достигают высоких показателей качества на задачах. Параллельно развиваются технологии AutoML, автоматизирующие выбор моделей и настройку гиперпараметров. Тем не менее, несмотря на прогресс отдельных компонентов, целостных универсальных AutoML-фреймворков, специально ориентированных на определение интентов пользователя, предложено немного. Существующие решения зачастую требуют участия эксперта для каждой новой предметной области, что указывает на необходимость разработать более обобщенный подход.

В связи с этим актуальной является проблема отсутствия универсального, масштабируемого и эффективного AutoML-решения для классификации интентов, способного автоматически адаптироваться к разным доменам без глубокого участия человека-эксперта.

*Цель исследования* заключается в разработке такого универсального AutoML-фреймворка, который способен автоматически подбирать оптимальные модели и их конфигурации для классификации интентов пользователя. Разработанное решение будет протестировано на различных корпусах данных (наборы пользовательских запросов), а его эффективность сопоставлена с результатами ручной настройки моделей, чтобы оценить выигрыш от автоматизации.

Для достижения поставленной цели в работе решены следующие задачи:
1. Провести обзор существующих фреймворков и библиотек, применяемых для построения моделей машинного обучения, включая решения для задач классификации пользовательских намерений;
2. Выполнить анализ современных алгоритмов и подходов к задаче определения пользовательских намерений, включая традиционные методы машинного обучения и нейросетевые архитектуры;
3. Разработать концепцию и архитектуру собственного фреймворка создания моделей машинного обучения для классификации пользовательских намерений;
4. Реализовать программную часть фреймворка с возможностью автоматизированной настройки моделей и выбора признаков;
5. Провести экспериментальное исследование эффективности фреймворка на нескольких датасетах из разных предметных областей;
6. Сравнить результаты, полученные с использованием разработанного фреймворка, с качеством моделей, настроенных вручную, и провести анализ полученных результатов.

*Практическая значимость* работы состоит в том, что созданный AutoML-фреймворк может быть непосредственно применен при разработке реальных диалоговых систем – чат-ботов, голосовых ассистентов, систем клиентской поддержки – и других NLP-приложений. Использование такого инструмента позволит ускорить внедрение новых сервисов и снизить порог вхождения для разработчиков за счет автоматизации подбора оптимальной модели под конкретный набор интентов.

*Научная новизна* исследования определяется интеграцией современных методов автоматизированного машинного обучения в единой специализированной архитектуре, ориентированной на задачу классификации интентов. В предлагаемом решении объединяются передовые подходы, включая трансформерные модели и методы обучения с малым количеством примеров, в рамках одного AutoML-фреймворка. Такое сочетание технологий нацелено на достижение высокой точности и устойчивости модели при минимальном ручном вмешательстве, что ранее не было реализовано в полной мере для задачи определения интентов пользователя.
* ОБЗОР ПРЕДМЕТНОЙ ОБЛАСТИ
** Определение намерений пользователя
Классификация намерений –  это задача сопоставления высказывания пользователя с предопределенной меткой намерения (семантической категорией цели пользователя). Например, запрос “Какая погода будет завтра?” может быть классифицирован как запрос погоды. Эта способность является ключевым компонентом понимания естественного языка (NLU) в диалоговых системах, позволяя чат-ботам, виртуальным помощникам и другим агентам искусственного интеллекта понимать, чего хочет пользователь, и соответствующим образом реагировать. Классификация намерений уходит корнями в ранние разговорные диалоговые системы (например, телефонное обслуживание клиентов) и с тех пор получила повсеместное распространение в самых разных областях - от личных помощников и ботов поддержки клиентов до систем медицинских и юридических консультаций.

Ранние методы были основаны на правилах, которые разрабатывались вручную, или на классическом машинном обучении с добавлением дополнительных функций. Однако с развитием области преобладать стали статистические методы, которые основываются на анализе данных. Сначала они использовали традиционные алгоритмы машинного обучения, а затем — методы глубокого обучения. Также мы наблюдаем расширение сферы применения: от простой классификации с закрытым набором параметров, когда каждый запрос должен относиться к одному из известных намерений, до более сложных сценариев. Например, к многоцелевой классификации, обнаружению намерений с открытым доменом или открытым набором параметров (когда запрос не соответствует ни одному из известных намерений), а также к распознаванию намерений с минимальным количеством попыток или вообще без них с помощью мощных генеративных моделей.
Классификация намерений – это задача сопоставления высказывания пользователя с предопределённой меткой, или интентом, отражающим семантическую цель запроса.  Например, запрос “Какая погода будет завтра?” может быть классифицирован как запрос погоды. Эта способность является ключевым компонентом понимания естественного языка (NLU) в диалоговых системах, позволяя чат-ботам, виртуальным помощникам и другим агентам искусственного интеллекта понимать, чего хочет пользователь, и соответствующим образом реагировать. Классификация намерений уходит корнями в ранние разговорные диалоговые системы (например, телефонное обслуживание клиентов) и с тех пор получила повсеместное распространение в самых разных областях - от личных помощников и ботов поддержки клиентов до систем медицинских и юридических консультаций.

Изначально системы классификации намерений строились на вручную заданных правилах и классических алгоритмах машинного обучения с набором признаков. С развитием технологий появились статистические методы и глубокие нейронные сети. Постепенно задачи усложнились – появились мульти-интентная классификация и открытые домены.

Также появилась проблема с запросами, которые не соответствуют ни одному из известных интентов (Out-of-Scope (OOS)) и требуют специальной обработки, чтобы избежать некорректных ответов.

Типичные примеры OOS-запросов: «Какой сейчас курс евро?» в погодном ассистенте, «Как оформить ипотеку?» в кино-ассистенте, «Расскажи, что я ел на прошлой неделе» в системе, не хранящей историю питания. При таких запросах система вежливо отказывается (<<Извините, я не могу помочь с этим запросом<<), предлагает альтернативы или перенаправляет в службу поддержки, а сами OOS-записи сохраняются для расширения набора интентов в будущем.
** Методы автоматического машинного обучения
Автоматизированное машинное обучение (AutoML) относится к автоматизации полного процесса применения методов машинного обучения для решения реальных задач. Вместо того чтобы вручную выбирать алгоритмы, настраивать гиперпараметры, разрабатывать архитектуры моделей и создавать признаки, система AutoML автоматически принимает эти решения на основе данных. Мотивация для развития AutoML вытекает из бурного роста применения машинного обучения и стремления "демократизировать" машинное обучение – сделать современные техники доступными даже для неспециалистов. Модели машинного обучения зачастую чувствительны к множеству параметров (тип модели, архитектура, настройки гиперпараметров, предварительная обработка признаков и так далее), и нахождение оптимальной конфигурации часто требует кропотливого перебора даже для экспертов. Эта проблема особенно заметна в глубоком обучении, где выбор правильной архитектуры сети и стратегии обучения может определять конечное качество модели. Цель AutoML – автоматизировать принятие этих решений, позволяя пользователю просто предоставить данные, а система подбирает оптимальную модель. Данный обзор литературы предоставляет академический анализ AutoML с основным упором на его применение в обработке естественного языка (NLP), а также включает как фундаментальные работы, так и последние разработки. Мы рассмотрим историческую эволюцию и мотивации AutoML, ключевые технические компоненты, ведущие фреймворки и системы, особенности применения AutoML в задачах NLP (например, классификация текстов, маркировка последовательностей, языковое моделирование), сравнительный анализ производительности и существующие бенчмарки, а также новые тенденции и направления исследований (например, интеграция с фундаментальными моделями, обучение с малым количеством примеров, объяснимость моделей). Обзор ссылается на рецензируемые публикации и академические источники.

*** H2O
H2O[cite:@ledell_h2o_2020] -- является платформой машинного обучения с открытым исходным кодом, разработанной для автоматизации процесса контролируемого обучения. Она ориентирована на выполнение таких задач, как бинарная классификация, многоклассовая классификация и регрессия. Основная функция AutoML в H2O заключается в обучении широкого спектра алгоритмов, включая градиентные повышающие машины, случайные леса, глубокие нейронные сети и обобщенные линейные модели. Затем эти модели объединяются в ансамбль для получения наилучших предсказаний. Результатом работы AutoML является таблица лидеров — ранжированный список моделей по показателям производительности, из которого можно выбрать оптимальную модель для развертывания.

Процесс автоматизации в H2O ограничен по времени, что позволяет пользователю задать максимальное время выполнения или количество моделей. Система обучает столько моделей, сколько возможно в рамках установленных ограничений. В отличие от более сложных методов оптимизации гиперпараметров (например, байесовской оптимизации), H2O использует случайный перебор моделей, полагаясь на разнообразие моделей и ансамблевую технику для достижения высокой производительности. Особенностью платформы является автоматическое создание двух сложенных ансамблей: один включает все обученные модели, другой — только лучшие модели каждого семейства алгоритмов. Этот подход позволяет повысить точность предсказаний без ручной настройки.

H2O эффективно обрабатывает большие массивы данных за счет распределения вычислений по нескольким ядрам или узлам кластера. Платформа реализована на языке Java и предоставляет API для Python, R и других языков, что позволяет интегрировать её в различные среды. Результаты работы легко интерпретируемы: на выходе формируется ранжированный список моделей с указанием показателей производительности и времени обучения. Кроме того, встроенные инструменты объяснения моделей позволяют пользователям получать такие пояснения, как важность переменных, графики частичной зависимости и значения SHAP для лучших моделей. Таким образом, H2O обеспечивает возможность построения множества моделей за короткий промежуток времени, что особенно актуально при работе с большими объемами данных.

*** LightAutoML
LightAutoML[cite:@vakhrushev_lightautoml_2022] (LAMA) — это облегчённый фреймворк AutoML с открытым исходным кодом, предназначенный для моделирования табличных данных. Основное назначение LAMA — автоматическая генерация конвейеров для структурированных данных с акцентом на скорость и эффективность использования вычислительных ресурсов. Он поддерживает задачи бинарной и многоклассовой классификации, а также регрессию. Изначально ориентированный на работу с табличными данными, LightAutoML расширил свои возможности и теперь поддерживает текстовые признаки.

Фреймворк автоматически выполняет предварительную обработку данных, включая очистку и кодирование отсутствующих значений, вывод типов признаков и их отбор в рамках конвейера. Гиперпараметры моделей настраиваются автоматически. LightAutoML предоставляет готовые пресеты конвейеров (например, "TabularAutoML"), которые обеспечивают быстрое развертывание моделей с минимальным вмешательством пользователя. Также доступны гибкие настройки для создания собственных конвейеров с учетом специфики задачи.

Отличительной чертой LightAutoML является параллельное обучение нескольких конвейеров, результаты которых объединяются с использованием ансамблевых методов. Это может быть простое усреднение или более сложное построение, при котором модели одного уровня используют предсказания предыдущего уровня в качестве входных данных. Также важной концепцией является разделение этапов чтения и предварительной обработки: компонент "Reader" проверяет исходный набор данных и определяет необходимые преобразования для различных типов признаков. Это гарантирует надежность и автоматизацию оценки модели.

*** AutoGluon
AutoGluon[cite:@erickson_autogluontabular_2020] -- комплексный инструментарий AutoML с открытым исходным кодом, который поддерживает широкий спектр задач машинного обучения, включая прогнозирование табличных данных, компьютерное зрение, обработку естественного языка и прогнозирование временных рядов. Фреймворк предоставляет специализированные API для каждой задачи, например, TabularPredictor и TextPredictor, что упрощает использование в различных областях.

AutoGluon поддерживает обучение различных моделей: от древовидных алгоритмов (LightGBM, XGBoost[cite:@chen_xgboost_2016], CatBoost[cite:@dorogush_catboost_2018]) до нейронных сетей (например, трансформеров для текста и сверточных сетей для изображений), а также простых моделей, таких как k-ближайших соседей и линейные модели. Пользователь может настроить гиперпараметры, выбрать конкретные модели для обучения и использовать предустановленные конфигурации. Таким образом, AutoGluon предоставляет гибкие возможности для настройки обучения с минимальным объемом кода.

*** FEDOT
FEDOT[cite:@nikitin_automated_2022; @polonskaia_multiobjective_2021](Flexible Evolutionary Design of Optimal Trees) -- фреймворк AutoML с акцентом на оптимизацию конвейеров с помощью эволюционных алгоритмов. Разработанный лабораторией моделирования природных систем Университета ИТМО, он предназначен для автоматизации полного жизненного цикла машинного обучения: от предварительной обработки данных до построения и оптимизации моделей.

Основная идея FEDOT -- создание составных конвейеров с помощью генетических алгоритмов. Конвейер представлен в виде направленного ациклического графа (DAG), узлы которого могут быть как преобразованиями данных, так и моделями. Эволюционный оптимизатор, известный как "GOLEM", генерирует начальную популяцию случайных конвейеров и затем улучшает их с помощью мутаций и скрещивания. В результате создаются оптимальные конвейеры, адаптированные к конкретной задаче.

FEDOT поддерживает работу с различными типами данных (табличные, текстовые, графовые) и обеспечивает гибкость настройки. Инструменты анализа позволяют исследовать чувствительность компонентов конвейера, а также оценивать влияние отдельных моделей на итоговую производительность. Фреймворк поддерживает экспорт оптимальных конвейеров в формате JSON и позволяет интеграцию в производственные среды.
*** Сравнение алгоритмов
Классификация намерений пользователя представляет собой важную задачу в области обработки естественного языка, требующую использования современных методов машинного обучения и автоматизированных инструментов для построения эффективных моделей. Для реализации данной задачи важно учитывать несколько ключевых критериев: способы обработки текста, поддержка работы с малым набором данных, поддержка выявления намерений вне области определения (Out-Of-Scope, OOS), гибкость настройки параметров, поддержка логирования и возможность использования промптов для энкодеров.

Первым важным критерием является обработка текста, поскольку текстовые данные являются основным источником информации при классификации намерений. Современные модели, такие как трансформеры, демонстрируют высокую точность в задачах NLP благодаря обучению на больших объемах текстов. Поэтому наличие встроенной поддержки текстовых признаков, включая возможность применения эмбеддингов и трансформерных архитектур, является важным аспектом при создании фреймворка.

Вторым значимым критерием является поддержка работы с малым набором данных. В прикладных задачах, связанных с классификацией намерений, часто возникает ситуация, когда количество размеченных данных ограничено. Это особенно актуально при адаптации моделей к новым доменам или редким языковым конструкциям. Поэтому важной характеристикой фреймворка является его способность эффективно работать с малыми наборами данных, например, за счет использования регуляризации или предварительно обученных эмбеддингов.

Не менее важной является поддержка Out-Of-Scope (OOS) -- задачи, заключающейся в выявлении запросов пользователя, выходящих за рамки известных классов намерений. Выявление OOS-классов критично для обеспечения надежности и безопасности систем, поскольку позволяет корректно обрабатывать неизвестные или неподдерживаемые запросы. Фреймворки, реализующие данную функциональность, позволяют обучать модели, способные распознавать не только заданные классы, но и детектировать аномальные данные.

Следующим важным аспектом является изменение параметров запуска. В зависимости от задачи, объема данных и доступных вычислительных ресурсов, может потребоваться гибкая настройка процесса обучения. Это особенно актуально при разработке моделей для различных доменов или на основе разнородных данных. Возможность адаптировать параметры позволяет оптимизировать модель как по точности, так и по времени выполнения.

Поддержка логирования является важным компонентом автоматизации машинного обучения, поскольку позволяет отслеживать процесс обучения, хранить промежуточные результаты и проводить анализ моделей. В контексте классификации намерений важно иметь возможность анализировать ошибки и проверять гипотезы о моделях на каждом этапе обучения. Логирование помогает выявлять причины ухудшения качества моделей и отслеживать процессы настройки гиперпараметров, что критично для обеспечения повторяемости экспериментов и объяснимости конечных результатов.

Последним критерием является поддержка промптов для энкодеров, что особенно важно при использовании моделей на основе трансформеров. В последнее появляются модели, которые поддерживают промпты в зависимости от задачи, которые улучшают качество ее работы. Например, модель e5[cite:@wang_multilingual_2024] использует ~query:~ и ~passage:~ для создания разных частей эмбеддинга для поиска похожих текста.

Таблица сравнения фреймворков по заданным критериям [[ref:tbl:automl_comparison]].

#+NAME: tbl:automl_comparison
#+CAPTION: Сравнение AutoML фреймворков
#+ATTR_LATEX: :align |p{3cm}|p{3cm}|p{3cm}|p{3cm}|p{3cm}| :placement [h!]
|------------------------------------------+-----------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------+-------------------------------------|
| Критерий                                 | H2O                                                 | LightAutoML                                                     | AutoGluon                                           | FEDOT                               |
|------------------------------------------+-----------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------+-------------------------------------|
| Способы обработки текста                 | Нет поддежки из коробки                             | TF-iDF[cite:@joneskarensparck_statistical_1972] и эмбеддинг     | Эмбеддинг                                           | TF-iDF, эмбеддинг                   |
|------------------------------------------+-----------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------+-------------------------------------|
| Поддержка работы с малым набором данных  | Не оптимизирован для малых данных                   | Имеет режимы, позволяющие работать с небольшими наборами данных | Нет поддержки                                       | Может адаптироваться к малым данным |
|------------------------------------------+-----------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------+-------------------------------------|
| Изменение параметров запуска             | Гибкая настройка через API                          | Настройка через пресеты и конфигурацию, плохо документировано   | Можно передавать свой конфиг, плохо документировано | Ограненная настройка                |
|------------------------------------------+-----------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------+-------------------------------------|
| Поддержка логирования во внешние системы | Логирование результатов через интеграцию с H2O Flow | Нет поддержки                                                   | Нет поддержки                                       | Нет поддержки                       |
|------------------------------------------+-----------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------+-------------------------------------|
| Поддержка промптов для энкодеров         | Нет поддержки                                       | Нет поддержки                                                   | Нет поддержки                                       | Нет поддежки                        |
|------------------------------------------+-----------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------+-------------------------------------|
| Поддержка OOS (out of scope)             | Нет встроенной поддержки                            | Нет встроенной поддержки                                        | Нет поддержки                                       | Нет поддержки                       |
|------------------------------------------+-----------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------+-------------------------------------|
** Нейросетевые методы представления текста
*** BERT
BERT[cite:@devlin_bert_2019] (Bidirectional Encoder Representations from Transformers) — это языковая модель на основе архитектуры трансформера[cite:@vaswani_attention_2017], которая предобучается на задаче маскированного языкового моделирования и предсказания следующего предложения. В отличие от односторонних моделей вроде GPT[cite:@radford_language_2019] или неглубоких двунаправленных конкатенаций, таких как ELMo[cite:@peters_deep_2018], BERT одновременно учитывает и левый, и правый контекст на всех слоях, что обеспечивает более глубокое понимание языка.

В своей базовой конфигурации модель содержит 12 «базовых блоков» (слоёв) трансформера, а размер скрытых представлений в каждом из них равен 768. Входной текст разбивается на токены с помощью WordPiece (словарь из 30 000 токенов), затем в начало последовательности добавляется специальный маркер =[CLS]=, а при подаче пары предложений между ними вставляется =[SEP]=. К каждому токену добавляются позиционные эмбеддинги и эмбеддинги сегментов, указывающие, к какому из предложений он относится. Представление токена =[CLS]= служит свёрнутым вектором для задач классификации, а остальные эмбеддинги используются для задачи span‐prediction.

Во время предобучения первая задача — маскирование токенов. 15 % токенов в каждом примере случайно выбирается для маскировки: 80 % из них заменяются на =[MASK]=, 10 % — на случайный токен, и 10 % остаются без изменений. Модель пытается угадать исходные токены, опираясь на полный контекст. Такая схема способствует более устойчивому обучению по сравнению с традиционными слева-направо или справа-налево моделями.

Вторая задача — предсказание следующего предложения: с вероятностью 50 % подаётся пара из действительно идущих друг за другом предложений, а с вероятностью 50 % — два случайных предложения из корпуса. Модель обучается определять, являются ли они смежными, что развивает понимание связности и логики текста (см. рис. [[fig:bert_pretrainin]]).

Для решения downstream‐задач BERT требует лишь добавления небольшой выходной головы: для классификации на токен =[CLS]=, для span‐prediction — двух векторов начала и конца и т. д. Затем все параметры модели дообучаются одновременно, что делает адаптацию универсальной и простой. Абляционные эксперименты показывают, что и двунаправленность внимания, и задача предсказания следующего предложения критически важны: при их исключении эффективность существенно падает, а увеличение глубины и ширины модели даёт стабильный прирост в переносимости представлений.

#+CAPTION: Пример преобразования входного текста в эмбеддинги
#+NAME: fig:inputemebeddings
#+ATTR_LATEX: :placement [h]
[[file:img/Input_Emebeddings.pdf]]

#+CAPTION: Сравнение направленностей внимания ELMo, GPT и BERT
#+NAME: fig:BERT_comparisons
#+ATTR_LATEX: :placement [h]
[[file:img/BERT_comparisons.pdf]]

#+CAPTION: Схема задачи предсказания следующего предложения в BERT
#+NAME: fig:bert_pretrainin
#+ATTR_LATEX: :width .6\textwidth :placement [h]
[[file:img/bert_pretrainin.png]]
*** Sentence Transformers
Sentence BERT[cite:@reimers_sentencebert_2019] (SBERT) -- модификаця исходной моделей BERT, нацеленная на эффективное построение векторных представлений предложений. В данной работе BERT выступает в роли общего кодировщика, параметры которого разделяются между двумя (или тремя, в случае триплетной версии) ветвями сети, обрабатывающими по отдельности входные предложения. Такое «сиамское» строение (biencoder) позволяет получать фиксированные векторы предложений, сохраняющие богатую семантическую информацию, без необходимости совместной обработки пар предложений на этапе инференса.

Основной этап обучения SBERT заключается в тонкой подгонке предобученного трансформера на разметках задач распознавания естественного вывода (SNLI, Multi-Genre NLI) или семантического сходства (STS). После прохождения каждого предложения через общий энкодер применяются операции агрегирования (mean-, CLS- или max-пулинг), формирующие итоговый эмбеддинг. Для оптимизации используются три различных критерия: классификационный (с дополнительным полносвязным слоем и softmax), регрессионный (минимизация MSE на косинусном сходстве) и триплетная функция потерь (гарантирующая, что «анкоры» ближе к «позитивам», чем к «негативам» на заданный порог).

В результате декомпозиции процедуры сравнения пар предложений и предварительного вычисления эмбеддингов SBERT демонстрирует существенное ускорение: поиск ближайших соседей в корпусе из 10 000 предложений, требовавший ранее десятков часов работы перекрёстного энкодера BERT на GPU, сводится к нескольким секундам при использовании SBERT и быстрых алгоритмов косинусного поиска. Это позволяет применять семантический поиск, кластеризацию и извлечение информации в режиме реального времени и на больших масштабах.

Стоит различать две парадигмы работы с парными входами в трансформерах. Cross-encoder принимает на вход конкатенацию двух предложений, обрабатывает их совместно и выдает прямую оценку сходства (или класс) через полносвязный классификатор — такая схема обеспечивает высочайшую точность, но накладывает квадратичную по размеру корпуса сложность инференса. Biencoder (сиамская или двухветвная модель) кодирует каждое предложение независимо в единое пространство эмбеддингов, после чего сходство вычисляется быстро «на лету» с помощью косинусной меры; это даёт компромисс между качеством и производительностью и лежит в основе SBERT.

** Методы адаптации моделей
*** P-Tuning
P-Tuning[cite:@liu_gpt_2023] дополняет дискретные подсказки обучаемыми непрерывными эмбеддингами, превращая их в гибридную схему, где модель может автоматически адаптировать ввод под специфику задачи. Вместо жёстко заданных шаблонов к входному тексту добавляется последовательность параметризованных векторов подсказок, которые проходят через облегчённый энкодер (LSTM, MLP или identity) и оптимизируются вместе с моделью или независимо от неё.

Метод решает проблему высокой нестабильности ручных дискретных подсказок, когда даже незначительные изменения формулировки приводят к резкому падению качества. Благодаря обучаемым эмбеддингам P-Tuning снижает разброс результатов при различных вариантах подсказок и позволяет получать более предсказуемый отклик модели.

P-Tuning демонстрирует значительный рост точности и устойчивости на широком спектре задач: от фактического знания (LAMA) до комплексных NLU-бенчмарков (SuperGLUE) и сценариев с ограниченным числом примеров. Этот подход обеспечивает более быструю и надёжную адаптацию моделей к новым задачам без затрат на полный перебор шаблонов.
*** LoRA
LoRA[cite:@hu_lora_2021] (Low-Rank Adaptation) -- метод обучения модели, который замораживает (не обучает) веса предобученной модели и обучает только небольшие низкоранговые матрицы обновлений, что позволяет существенно сократить количество обучаемых параметров и требования к хранению при сохранении полной скорости инференса.

LoRA решает проблему высокой стоимости тонкой настройки всё более крупных моделей на основе трансформеров, при которой обновление всех параметров требует огромных ресурсов GPU. Вместо изменения исходной матрицы весов $W_0$, LoRA представляет адаптацию $\Delta W$ как произведение двух значительно меньших матриц, используя тот факт, что эффективные обновления лежат в низкоразмерном подпространстве.

Конкретно, для полносвязанного слоя с \(W_0 \in \mathbb{R}^{d \times k},\) LoRA вводит матрицу \(\Delta W = B\,A,\) где \(A \in \mathbb{R}^{r \times k},\quad B \in \mathbb{R}^{d \times r},\quad r \ll \min(d,k)\). Обучаются только $A$ и $B$ (инициализируемые так: $A \sim \mathcal{N}(0,\sigma^2)$, $B = 0$), в то время как $W_0$ остаётся неизменным. Скалярный множитель $\tfrac{\alpha}{r}$ масштабирует обновление для стабилизации обучения. Во время работы матрица считается как \(h = W_0\,x + (B\,A)\,x\).

LoRA совместим с другими методами повышения эффективности: в отличие от адаптеров, добавляющих новые слои, или prompt-tuning, расширяющего входную последовательность, он не увеличивает вычислительную сложность и не снижает максимальную длину обрабатываемых последовательностей.
** Методы классификации текста
*** Logistic Regression
Логистическая регрессия — это статистический метод, используемый для моделирования вероятности двоичного исхода (например, успех/неудача) на основе одного или нескольких предикторов. Она преобразует линейную комбинацию признаков через логистическую (сигмоидную) функцию

$$
\sigma(z)=\frac{1}{1+e^{-z}},
$$

гарантируя, что предсказанные значения лежат между 0 и 1 и могут интерпретироваться как вероятности. В этой модели логарифм отношения шансов («логит») задаётся линейно:

$$
\log\frac{\Pr(Y=1\mid \mathbf{x})}{\Pr(Y=0\mid \mathbf{x})} = \beta_0 + \sum_{i=1}^p \beta_i x_i.
$$

Параметры оцениваются методом максимального правдоподобия: выбираются такие коэффициенты, которые максимизируют вероятность наблюдать имеющиеся данные при заданной модели. Так как функция лог-правдоподобия выпукла относительно коэффициентов, алгоритмы вроде метода Ньютона или градиентного подъёма надёжно сходятся к глобальному оптимуму. Оценка коэффициента $\beta_i$ интерпретируется так: при увеличении $x_i$ на единицу шансы наступления события умножаются на $\exp(\beta_i)$. Для классификации новых наблюдений вычисляют сигмоиду от линейного выражения и применяют порог (обычно 0.5): выше — класс «1», ниже — класс «0».

Логистическая регрессия ценится за простоту, интерпретируемость и способность работать как с непрерывными, так и с категориальными признаками. Она выступает надёжным базовым методом в задачах классификации — от медицинской диагностики до прогнозирования оттока клиентов в маркетинге — и её эффективность оценивается такими метриками, как точность, precision/recall, F1-мера и ROC-AUC. Главный недостаток модели — предположение о линейной зависимости между предикторами и логарифмом шансов; при его нарушении можно добавить перекрёстные и полиномиальные признаки или обратиться к более гибким методам.
*** ML-KNN
ML-kNN[cite:@zhang_mlknn_2007](многометочный k-ближайших соседей) — это ленивый алгоритм, расширяющий традиционный kNN для задач многометочной классификации. В многометочной постановке каждый объект может принадлежать нескольким категориям одновременно. ML-kNN предсказывает набор меток для нового объекта, анализируя его ближайших соседей в обучающей выборке и применяя вероятностное правило принятия решения на основе статистики совместного появления меток.

1. Представление меток и подсчет вхождений

   Пусть $Y = {1, 2, …, Q}$ — множество всех возможных меток. Каждый объект x представлен бинарным вектором категорий, где $y_x(l) = 1$, если метка l принадлежит x, и 0 в противном случае. Для данного $x$ ML-kNN находит $k$ ближайших соседей $N(x)$ и строит вектор подсчёта вхождений $C_x$, чей \(l\)-й компонент вычисляется как
   \[
    \tilde C_x(l) = \sum_{a \in N(x)} \tilde y_a(l)
  \]

2. Оценка априорных и апостериорных вероятностей (этап обучения)

   На этапе обучения ML-kNN рассматривает каждую метку l независимо и оценивает:
   - Априорные вероятности \(P(H_l^1)\) и \(P(H_l^0) = 1 - P(H_l^1)\), где \(H_l^1\) обозначает событие, что случайный объект имеет (не имеет) метку \(l\).
   - Условные вероятности \(P(E_l^j \mid H_l^b)\), где \(E_l^j\) — событие того, что ровно \(j\) из \(k\) соседей имеют метку \(l\), а \(b \in \{0,1\}\).

3. Предсказание по следующему правилу:

   Для каждого тестового объекта \(t\) ML-kNN сначала находит \(K\) ближайших соседей \(N(t)\) в обучающей выборке. Пусть \(H_l^1\) — событие, что \(t\) имеет метку \(l\), а \(H_l^0\) — событие, что \(t\) не имеет метки \(l\). Обозначим \(E_l^j\) (\(j\in\{0,1,\dots,K\}\)) событие, что среди \(K\) ближайших соседей \(t\) ровно \(j\) объектов имеют метку \(l\). Тогда на основе вектора подсчёта вхождений \(\tilde C_t\) вектор категорий \(\tilde y_t\) определяется по принципу:
   \[
   \tilde y_t(l) \;=\; \arg\max_{b\in\{0,1\}}
   P\bigl(H_l^b \mid E_l^{\tilde C_t(l)}\bigr),
   \quad l\in Y.
   \]
4. Ранжирование меток
   Помимо бинарного предсказания $y_t$, ML-kNN вычисляет вещественный вектор ранжирования $r_t$, где для каждой $l$:
   \[
    \tilde r_t(l)
    = P\bigl(H_l^1 \mid E_l^{\tilde C_t(l)}\bigr)
    \]
   Это ранжирование позволяет отбирать метки по порогу.
*** DNNC
Discriminative Nearest Neighbor Classification[cite:@zhang_discriminative_2020] (DNNC) реализуется как попарная функция соответствия: входное высказывание пользователя и эталонный пример соединяются в единую последовательность и обрабатываются с помощью BERT-подобной модели. На выходе текстовый векторы преобразуются с помощью функции, которая оценивает вероятность совпадения намерений пары. Во время работы выбирается эталон с максимальным значением вероятности, после чего применяется порог для разграничения известных намерений и OOS-запросов.

Для снижения вычислительной нагрузки при большом количестве эталонных примеров предложен двухэтапный «совместный» (joint) механизм: сначала применяется более лёгкий метод отбора для выбора кандидатов, далее глубокая попарная модель DNNC доранжирует только отобранный набор. Данный приём сохраняет высокую дискриминативную способность при существенно уменьшенных требованиях к времени обработки.
*** CatBoost
CatBoost[cite:@dorogush_catboost_2018;@prokhorenkova_catboost_2018] — это библиотека градиентного бустинга над решающими деревьями, которая изначально поддерживает работу с категориальными признаками без обширной предварительной обработки. В отличие от традиционных реализаций градиентного бустинга, CatBoost использует такие техники, основанныt на пермутационной статистике для предотвращения утечки целевых значений, и симметричные (обоюдные) деревья для снижения переобучения и повышения как стабильности, так и вычислительной эффективности.

При обработке текстовых признаков CatBoost использует многоступенчатый алгоритм, преобразующий строки в числовые векторы, пригодные для деревьев градиентного бустинга. Сначала текстовые столбцы загружаются, после чего каждая запись разбивается на токены — слова, символы или настраиваемые n-граммы. Затем строится словарь, в котором каждому уникальному токену присваивается числовой идентификатор. Каждая текстовая запись преобразуется в последовательность и передаётся на вход другим алгоритмам, которые вычисляют числовые сводки — индикаторы наличия токенов, условные вероятности по классам или оценки релевантности. Полученные признаки интегрируются в стандартный процесс обучения CatBoost.

Для признаков-эмбеддингов, представленных в виде фиксированных числовых векторов, CatBoost также генерирует скалярные признаки перед обучением деревьев. После указания таких столбцов поддерживаются два основных метода обработки. Линейный дискриминантный анализ (LDA) проецирует эмбеддинги в пространство низкой размерности и вычисляет для каждого класса значения гауссовой функции правдоподобия (для классификации), а метод ближайших соседей (KNN) определяет ближайшие векторы из тренировочного набора, подсчитывая вхождения по классам или усредняя целевые значения соседей (для регрессии или классификации). Такие признаки, учитывающие информацию о классах или целевых значениях, позволяют CatBoost эффективно использовать семантику эмбеддингов без прямой работы с высокоразмерными координатами — хотя сами векторы можно добавить как обычные числовые признаки при необходимости.
** Методы поиска текста
Поиск сходства векторов стал одной из ключевых операций в современных системах ИИ, когда самые разные данные — от слов и предложений до изображений и взаимодействий пользователей с контентом — отображаются в высокоразмерные эмбеддинги, в которых геометрическая близость отражает семантическое сходство. Это требует разработки высокоэффективных алгоритмов, способных обеспечить баланс между точностью, скоростью и объемом требуемой памяти. В частности, методы аппроксимационного поиска ближайших соседей (ANNS) стали незаменимыми в тех сценариях, где точный перебор оказывается неприемлемо затратным по времени.

Faiss[cite:@douze_faiss_2025] представляет собой набор инструментов, который сосредоточен исключительно на ядре ANNS: он не занимается извлечением эмбеддингов и не предоставляет сервисы управления базами данных, такие как транзакции или планирование запросов. Вместо этого Faiss предлагает богатый набор индексирующих примитивов с настраиваемыми параметрами, которые можно комбинировать, создавая специализированные алгоритмы поиска. Начиная от простых плоских индексов и заканчивая сложными многоступенчатыми структурами, Faiss позволяет пользователям оптимизировать решение под свои требования по скорости, точности и ресурсам.

Для быстрого поиска по большим коллекциям векторов Faiss реализует две взаимодополняющие стратегии, не требующие полного перебора. Индексы с инвертированным файлом (IVF) группируют базу данных на настраиваемое число «списков» и при выполнении запроса обрабатывают лишь их часть; при этом остаточное (residual) кодирование после грубого квантования повышает точность. Графовые методы, такие как Hierarchical Small Navigable World (HNSW)[cite:@malkov_efficient_2018], строят навигируемые маломировые графы для эффективного поиска соседей.
** Способы расширения данных
#+begin_comment
[cite:@li_generating_2024]
- Intent-augmentation [cite:@hu_exploring_2024]
- Few-shot detection [cite:@hou_fewshot_2021]
- Dspy [cite:@khattab_dspy_2023]
#+end_comment

В работе [cite:@li_generating_2024] уделили внимание критическому недостатку систем диалогов с задачами: склонности классификаторов намерений к ошибкам при встрече с очень похожими текстами (hard-negatives) внеобласти (OOS) высказываниями, которые похожи на поддерживаемые интенты, но на самом деле выходят за рамки домена системы. Авторы представляют полностью автоматизированный алгоритм на базе ChatGPT: сначала выделяют <<важные>> слова для каждого интента, затем генерируют OOS-примеры, включающие эти слова, и на последнем шаге с помощью двухступенчатой проверки GPT убеждаются, что полученные высказывания действительно не соответствуют ни одному поддерживаемому интенту. Применив этот подход к пяти наборам данных, они сформировали 3 732 таких высказываний. При оценке оказалось, что модели, обученные только на доменных данных, слишком самоуверенны на этих похожих примерах, но включение сгенерированных высказываний в тренировочный набор резко улучшает метрики.

@@latex:\textcolor{red}{Переписать немного обращения к работам}@@
В работе [cite/text:@hu_exploring_2024] схожим образом возвращаются к задаче классификации намерений без дополнительного обучения, используя текстовые эмбеддинги, чтобы обойтись без каких-либо размеченных примеров. Они предлагают несколько схем дополнения простого подхода косинусного сходства описаниями интентов — короткими декларированиями, сохраняющими ключевые слова из названий интентов (например, «BookRestaurant» превращается в «пользователь хочет забронировать столик в ресторане»).

Для автоматизации расширерния данных можно использовать библиотеку DSPy[cite:@khattab_dspy_2023] (Declarative Self-improving Python). Она представляет собой Python-фреймворк для декларативного описания взаимодействия с языковыми моделями и их автоматической оптимизации. В отличие от традиционных подходов, где разработчик вручную конструирует многослойные шаблоны промптов, dspy формирует граф текстовых преобразований, в котором каждый узел задаётся через формальную сигнатуру входов и выходов.

Архитектура dspy опирается на три центральные абстракции:
1. Сигнатуры, определяющие контракт модуля путём спецификации типов и форматов входных и выходных параметров;
2. Модули, инкапсулирующие распространённые техники промптинга и работу с внешними инструментами (=Chain-of-Thought=, =few-shot= и другие) в виде параметризуемых компонентов;
3. Телепромптеры (teleprompters), автоматически подбирающие демонстрации и инструкции на основе набора «учебных» примеров и заданной метрики, а при необходимости оптимизирующие параметры модели.

Оптимизационный процесс dspy заключается в итеративном исполнении <<учебных>> примеров через исходный конвейер (режим <<учителя>>), сборе успешных траекторий работы модулей и отборе наиболее эффективных демонстраций и инструкций. По результатам этой фазы возвращается оптимизированная декларативная программа, готовая к промышленному использованию.
** Метрики для оценки качества алгоритма
*** Метрики поиска
Эти метрики используются для оценки качества систем поиска и рекомендаций, которые возвращают ранжированный список документов или элементов. Поскольку пользователям важнее получить релевантные ответы на первых позициях, метрики ранжирования показывают, насколько хорошо система выводит нужные объекты вверху. С их помощью можно сравнивать разные алгоритмы, подбирать оптимальные параметры и отслеживать прогресс при обучении моделей.


1. Precision@k

   Precision@k показывает, какую долю из первых $k$ результатов составляют релевантные документы:
   $$
      P@k = \frac{1}{k}\sum_{i=1}^{k}\mathrm{rel}_i,
   $$
   где $\mathrm{rel}_i$ равно 1, если документ на позиции $i$ релевантен, и 0 — иначе. Эта метрика проста и интуитивно понятна, что является её сильной стороной: она прямо отражает практическую пользу выдачи при просмотре первых $k$ ответов. Однако P@k игнорирует порядок внутри первых $k$ (то есть один релевантный документ на 1-й позиции и на \(k\)-й считаются одинаковыми) и полностью не учитывает результаты после \(k\)-го, что может приводить к переоценке алгоритмов, которые хорошо работают только на небольшом числе верхних позиций.

2. NDCG@k (Normalized Discounted Cumulative Gain)

   NDCG@k учитывает и степень релевантности (градуированную оценку), и штрафует более низкие позиции:
   $$
   \mathrm{DCG}_k = \sum_{i=1}^{k}\frac{2^{\mathrm{rel}_i}-1}{\log_2(i+1)},\qquad
   \mathrm{NDCG}_k = \frac{\mathrm{DCG}_k}{\mathrm{IDCG}_k},
   $$

   где $\mathrm{IDCG}_k$ -- максимальное возможное значение DCG при идеальном ранжировании. Благодаря учёту логарифмического дисконтирования NDCG снижает вклад документов, появившихся дальше, а использование $2^{\mathrm{rel}_i}-1$ усиливает вклад особо релевантных материалов. Это делает NDCG гибкой и информативной: она отражает разницу между «очень» и «слабо» релевантными документами, но одновременно более сложна в вычислении и интерпретации, чем P@k, и требует наличия градуированных меток релевантности.

3. MAP (Mean Average Precision)

   MAP усредняет точность с учётом позиций всех релевантных документов и затем берёт среднее по запросам. Сначала для каждого запроса вычисляют
   $$
   \mathrm{AP}=\frac{1}{R}\sum_{i=1}^{n}P@i\;\mathrm{rel}_i,
   $$
   где $R$ -- общее число релевантных документов для запроса, а $n$ -- рассматриваемая длина выдачи. Затем
   $$
   \mathrm{MAP} = \frac{1}{|Q|}\sum_{q\in Q}\mathrm{AP}_q.
   $$
   MAP хорошо отражает ранжирование в целом, поскольку чем раньше появляются релевантные, тем выше значение AP, и при этом учитываются все такие документы. Однако она не подходит для градуированных оценок и зависит от того, сколько релевантных документов существует и до какого $n$ мы считаем выдачу, что усложняет сравнение моделей на разных наборах данных.

4. MRR (Mean Reciprocal Rank)

   MRR показывает, как быстро в среднем находится первый релевантный документ. Для каждого запроса берут обратную величину ранга первого релевантного результата $\mathrm{rank}_q$:
   $$
   \mathrm{RR}_q = \frac{1}{\mathrm{rank}_q},\qquad
   \mathrm{MRR} = \frac{1}{|Q|}\sum_{q\in Q}\mathrm{RR}_q.
   $$
   Эта метрика отличается простотой и прозрачностью: она сразу показывает, на какой позиции в среднем появляется первый релевантный ответ. С другой стороны, MRR игнорирует все релевантные документы после первого, поэтому не отражает полноту выдачи и может быть неинформативна, если для пользователя важны не только первые найденные, но и последующие релевантные результаты.

*** Метрики классификации
Метрики, описанные в данном пункте, применяются при оценке классификаторов и помогают понять, насколько точно модель определяет положительный и отрицательный классы, а также насколько она сбалансирована при разных соотношениях классов. С их помощью можно выбирать лучшее пороговое значение и сравнивать алгоритмы.

Для наглядного представления результатов классификации служит матрица ошибок (confusion matrix), в которой по строкам указаны предсказания модели $f(x)$, а по столбцам — истинные значения $y$. Эта таблица позволяет сразу увидеть, сколько примеров модель правильно и неправильно классифицировала:

|------------+-------------------------+-------------------------|
|            | $Y = 0$ (Отрицательный)   | $y = 1$ (Положительный)   |
|------------+-------------------------+-------------------------|
| $f(x) = 0$   | TN                      | FN                      |
|------------+-------------------------+-------------------------|
| $f(x) = 1$   | FP                      | TP                      |
|------------+-------------------------+-------------------------|
Где
- TN (True Negative) -- модель правильно предсказала отрицательный класс;
- FN (False Negative) -- модель ошибочно отнесла положительный пример к отрицательному;
- FP (False Positive) -- модель ошибочно отнесла отрицательный пример к положительному;
- TP (True Positive) -- модель правильно предсказала положительный класс.

На основе элементов матрицы ошибок можно вычислить ряд ключевых метрик:

1. Accuracy (доля верных классификаций)

   $$
   \mathrm{Accuracy} = \frac{\mathrm{TP} + \mathrm{TN}}{\mathrm{TP} + \mathrm{TN} + \mathrm{FP} + \mathrm{FN}},
   $$
   где TN (true negatives) -- число правильно определённых отрицательных примеров. Accuracy отражает общую долю правильных ответов модели и проста для интерпретации, однако на сильно несбалансированных данных она может вводить в заблуждение: модель, предсказывающая всегда «отрицательный», при 99 % отрицательных примерах получит 99 % точности, хотя фактически будет бесполезна.

2. Precision (точность предсказания положительного класса)

   $$
   \mathrm{Precision} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}},
   $$
   где TP (true positives) -- число верно предсказанных положительных примеров, а FP (false positives) — количество ложно «положительных». Precision показывает, какую долю среди предсказанных моделью «положительных» примеров составляют действительно положительные. Это важно, когда ложные срабатывания дорого обходятся (например, спам-фильтр не должен блокировать важные письма). При этом Precision игнорирует все пропущенные положительные примеры (FN), поэтому модель, слишком консервативно отмечающая положительные случаи, может иметь высокий Precision при очень низком Recall.

3. Recall (полнота, чувствительность)

   $$
   \mathrm{Recall} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}},
   $$

   где FN (false negatives) -- число пропущенных моделью положительных примеров. Recall показывает, какую долю от всех истинно положительных примеров модель смогла обнаружить, что актуально, когда важно не упустить ни одного положительного случая (например, при диагностике заболеваний). Достоинство этой метрики — фокус на захват всех «плюсов», однако она не учитывает ложно положительные срабатывания, и высокая Recall может достигаться ценой большого числа FP.

4. F1-score

   $$
   \mathrm{F1} = 2 \times \frac{\mathrm{Precision} \times \mathrm{Recall}}{\mathrm{Precision} + \mathrm{Recall}}.
   $$

   F1 объединяет точность и полноту, отдавая больше веса тем случаям, когда одна из метрик низка, и тем самым обеспечивает сбалансированную оценку работы модели при неоднородных классах. Это полезно, когда важно одновременно и не пропускать положительные примеры, и не допускать много ложных срабатываний. Однако F1 не учитывает TN и потому не отражает способность модели правильно распознавать отрицательные примеры; кроме того, оно предполагает равный вес Precision и Recall, что не всегда соответствует бизнес-целям.

5. ROC AUC

   ROC-кривая строится по точкам (FPR, TPR), где

   $$
   \mathrm{TPR} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}},\quad
   \mathrm{FPR} = \frac{\mathrm{FP}}{\mathrm{FP} + \mathrm{TN}},
   $$

   а AUC -- это интеграл под этой кривой. Высокий ROC AUC означает, что модель хорошо различает положительные и отрицательные примеры при любом пороге, что делает её независимой от выбора порога и удобной для сравнения алгоритмов. С другой стороны, при сильном дисбалансе классов AUC может давать искажённо высокую оценку, поскольку учитывает весь диапазон порогов, включая нерелевантные для прикладных задач точки, и не показывает, как модель ведёт себя при конкретных настройках.

* Реализация
** Архитектура
В рамках этой работы построен фреймворк AutoIntent для классификации намерений и состоит из последовательности из трех основных типов узлов: Embedding (векторные представления), Scoring (оценка) и Decision (принятие решения). Каждый узел в этом конвейере инкапсулирует определённый этап обработки и может быть создан с использованием одного из нескольких подключаемых модулей, реализующих логику этого этапа. Такой подход обеспечивает чёткое разделение ответственности: например, векторизация текста осуществляется в специализированном модуле embedding, а машинно-обучаемые классификаторы работают в модулях scoring. Конвейер собирается и оптимизируется автоматически: оптимизатор перебирает различные варианты модулей и гиперпараметры для каждого узла, стремясь максимизировать выбранную метрику оценки. Фреймворк делает упор на настраиваемость и повторное использование заранее обученных моделей для создания текстовых эмбеддингов, одновременно автоматизируя поиск наилучшей конфигурации конвейера. Он также обеспечивает надёжную поддержку как многоклассовой, так и мульти-классификации интентов, а также обработку OOS запросов.

Дизайн системы основан на данных и методах поиска. Имея размеченный набор данных, оптимизатор конвейера инициализирует каждый узел кандидатами-модулями и настраивает их параметры с помощью Optuna и стратегий полного перебора. Для поиска могут использоваться разные методы выборки — например, TPE (Tree-structured Parzen Estimator) или случайный поиск — чтобы эффективно исследовать пространство гиперпараметров. AutoIntent применяет объект контекста для управления набором данных, конфигурациями и результатами в ходе этого поиска. Во время оптимизации каждый кандидат-модуль обучается и оценивается на датасете (с использованием либо отложенной валидационной выборки, либо перекрёстной проверки, в зависимости от настроек), и его показатели производительности сохраняются. Лучший по показателям модуль для каждого узла выбирается и сохраняется как часть финального конвейера. Этот процесс автоматического отбора по сути представляет собой разновидность поиска архитектур нейронных сетей и настройки гиперпараметров: фреймворк способен выбирать между принципиально разными архитектурами моделей (к примеру, логистическая регрессия или модель на базе BERT) и находить для каждой оптимальные настройки. Принцип здесь таков: рассматривать сборку классификатора интентов как задачу поиска, разбивая её на подкомпоненты и оптимизируя каждый по отдельности.

Ещё одним ключевым аспектом является модульность и расширяемость. Все модули соответствуют общим абстрактным базовым классам, гарантируя, что они предоставляют единый интерфейс для обучения, предсказания и оценки. Это позволяет конвейеру «подменять» модули во время поиска AutoML. Новые алгоритмы могут быть интегрированы как дополнительные модули с минимальными изменениями в общей логике конвейера. Использование системы контекста и конфигурации означает, что такие детали, как выбор модели-трансформера для эмбеддингов или способ токенизации входов, задаются через объекты конфигурации, что соответствует принципу разделения конфигурации и реализации. AutoIntent также делает упор на воспроизводимость экспериментов и отслеживание: он предоставляет утилиты логирования для записи результатов каждого запуска и может сохранять оптимизированный конвейер (включая веса моделей и параметры) в каталоге запуска для последующего запуска. В целом архитектура сочетает гибкость (благодаря поддержке широкого спектра методов и настроек) с автоматизацией (за счёт поиска оптимальной комбинации этих методов).

#+NAME: fig:framework_schema
#+begin_src mermaid :file img/mermaid/framework_schema.png :results output :theme neutral :scale 5
%%{
   init: {
     "theme": 'base',
     "themeVariables": {
       "primaryColor": '#FFF',
       "primaryTextColor": '#000',
       "primaryBorderColor": '#000',
       "lineColor": '#000'
     }
   }
}%%
flowchart TB
    data[Данные]
    config[Конфигурация]
    pipeline[Пайплайн оптимизации]
    params[Параметры]
    block[Блок]
    scoring[Оценка]
    select_best[Выбор лучших параметров]

    data --> pipeline
    config --> pipeline
    pipeline --> params
    params --> block
    block --> scoring
    scoring --> select_best
    select_best --> pipeline
#+end_src

#+CAPTION: Схема фреймворка
#+ATTR_LATEX: :width 0.6\textwidth :height 0.5\textheight :placement [h]
#+RESULTS: fig:framework_schema
[[file:img/mermaid/framework_schema.png]]


** Данные
AutoIntent использует абстракцию Dataset для управления обучающими, валидационными и тестовыми разделами. Пользователь предоставляет входные данные в формате JSON, включая при необходимости отдельные валидационные и тестовые выборки, а фреймворк может также загружать стандартные датасеты из централизованного хаба. Внутри системы объект-контекст отвечает за разбиение данных в соответствии с выбранной схемой — либо выделение отложенной выборки (hold-out), либо k-кратная перекрёстная проверка (cross-validation). Также можно управлять количеством примеров в классах во время обучения с помощью параметра few-shot, что позволит избежать переобучения под самые распространённые классы.
** Конфигурация
** Context
** Модули
*** Encoder
*** Scoring
*** Decision
** Обучение
** Logging
** Тестирование
** CI
* Эксперименты

- Сравнение с фреймворкам по пресетам
- Сравнение модулей
- Сравнение энкодеров и промтов
- Сравнение few-shot
- Сравнение качества на аугментированных данных
* Заключение
:PROPERTIES:
:UNNUMBERED: t
:END:
#+print_bibliography: :title СПИСОК\spaceИСПОЛЬЗОВАНЫХ\spaceИСТОЧНИКОВ
* TODO Старые схемы :noexport:
#+begin_src mermaid :file img/mermaid/optimization_schema.png :results output :theme neutral :scale 5
%%{
   init: {
     "theme": 'base',
     "themeVariables": {
       "primaryColor": '#FFF',
       "primaryTextColor": '#000',
       "primaryBorderColor": '#000',
       "lineColor": '#000'
     }
   }
}%%
flowchart TD
    config[Конфигурация] --> optimizator[Оптимизатор]
    optimizator --> params[Параметры]
    params --> node[Блок]
#+end_src

#+RESULTS:
[[file:img/mermaid/optimization_schema.png]]
* Footnotes

[fn:1] https://www.verloop.io/blog/100-best-chatbot-statistics
# Local Variables:
# org-latex-title-command: nil
# org-latex-packages-alist: nil
# org-latex-listings: t
# org-latex-toc-command: "\\MyTOC\n\n"
# org-latex-pdf-process: ("latexmk -f -xelatex -%latex -interaction=nonstopmode -output-directory=%o %f")
# End:
